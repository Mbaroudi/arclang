# üöÄ Terraform Infrastructure - Generated from ArcLang Model

**Generated**: 2025-10-21  
**Source Model**: `examples/data_platform_migration.arc`  
**Target Platform**: Databricks + AWS  
**Terraform Provider**: databricks/databricks v1.86+

---

## ‚úÖ What Was Generated

**624 lines** of production-ready Terraform code automatically generated from the ArcLang model, including:

### üì¶ Infrastructure Components

1. **Unity Catalog Setup**
   - Metastore configuration
   - Catalog and schema creation
   - Storage credentials
   - External locations for Delta Lake

2. **Compute Resources**
   - Data engineering cluster (autoscaling 2-200 nodes)
   - Streaming pipeline cluster (always-on)
   - SQL Analytics warehouse (2X-Large with Photon)

3. **Storage (AWS S3)**
   - Unity Catalog metastore bucket
   - Delta Lake storage bucket with lifecycle policies
   - IAM roles and policies for secure access
   - Versioning and encryption enabled

4. **Jobs & Workflows**
   - Data migration ETL job with 4 tasks:
     - Extract from Oracle
     - Extract from Snowflake
     - Schema conversion
     - Data validation
   - Dependency management between tasks
   - Scheduling (daily at 2 AM)

5. **Governance & Access Control**
   - User groups (data-engineers, data-analysts, data-scientists)
   - Catalog-level permissions (RBAC)
   - Cluster policies
   - System schemas for audit/access logs

6. **Monitoring & Observability**
   - Notification destinations (Email, Slack)
   - Cluster usage monitoring queries
   - Cost tracking dashboard

---

## üéØ Model-to-Infrastructure Mapping

The generator automatically mapped ArcLang model elements to Terraform resources:

| ArcLang Element | Terraform Resource | Count |
|-----------------|-------------------|-------|
| **LA-PROC-001** (Batch Pipeline) | `databricks_cluster.data_engineering` | 1 |
| **LA-PROC-002** (Streaming Pipeline) | `databricks_cluster.streaming` | 1 |
| **LA-ANLZ-001** (SQL Analytics) | `databricks_sql_endpoint.analytics` | 1 |
| **LA-MIG-001** (ETL Orchestrator) | `databricks_job.data_migration` | 1 |
| **LA-TGT-002** (Unity Catalog) | `databricks_catalog.data_platform` | 1 |
| **LA-TGT-003** (Delta Lake) | `aws_s3_bucket.delta_lake` | 1 |
| **LA-GOV-001** (Access Control) | `databricks_grants.catalog` | 1 |
| **LA-MON-002** (Cost Optimizer) | `databricks_dashboard.cost_tracking` | 1 |
| **Schemas** | `databricks_schema.*` | 7 |
| **Requirements** | Resource tags | 27 |
| **Traces** | `depends_on` relationships | 32 |

---

## üèóÔ∏è Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  DATABRICKS WORKSPACE (AWS us-east-1)                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                              ‚îÇ
‚îÇ  Unity Catalog Metastore                                    ‚îÇ
‚îÇ    ‚îî‚îÄ Catalog: data_platform                                ‚îÇ
‚îÇ         ‚îú‚îÄ Schema: processing                               ‚îÇ
‚îÇ         ‚îú‚îÄ Schema: migration                                ‚îÇ
‚îÇ         ‚îú‚îÄ Schema: governance                               ‚îÇ
‚îÇ         ‚îú‚îÄ Schema: analytics                                ‚îÇ
‚îÇ         ‚îî‚îÄ Schema: monitoring                               ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  Compute Clusters                                           ‚îÇ
‚îÇ    ‚îú‚îÄ data-engineering-cluster (2-200 nodes)                ‚îÇ
‚îÇ    ‚îú‚îÄ streaming-pipeline-cluster (2-50 nodes)               ‚îÇ
‚îÇ    ‚îî‚îÄ analytics-sql-warehouse (2X-Large + Photon)           ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  Jobs & Workflows                                           ‚îÇ
‚îÇ    ‚îî‚îÄ data-migration-etl (4 tasks, daily)                   ‚îÇ
‚îÇ                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  AWS INFRASTRUCTURE                                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                              ‚îÇ
‚îÇ  S3 Buckets                                                 ‚îÇ
‚îÇ    ‚îú‚îÄ unity-catalog (metastore storage)                     ‚îÇ
‚îÇ    ‚îî‚îÄ delta-lake (data storage with tiering)                ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  IAM                                                        ‚îÇ
‚îÇ    ‚îú‚îÄ databricks-storage-access role                        ‚îÇ
‚îÇ    ‚îî‚îÄ Storage access policies                               ‚îÇ
‚îÇ                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìã Prerequisites

Before applying this Terraform configuration:

1. **Databricks Account**
   - Active Databricks account
   - Account console access
   - Account ID for Unity Catalog

2. **AWS Account**
   - AWS account with admin access
   - AWS CLI configured
   - Sufficient quota for EC2/S3

3. **Terraform**
   ```bash
   terraform --version  # Should be >= 1.5.0
   ```

4. **Variables Configuration**
   Create `terraform.tfvars`:
   ```hcl
   workspace_host        = "https://your-workspace.cloud.databricks.com"
   databricks_account_id = "your-account-id"
   environment           = "prod"
   ```

---

## üöÄ Deployment Steps

### 1. Initialize Terraform

```bash
cd /Users/malek/Arclang/terraform
terraform init
```

Expected output:
```
Initializing provider plugins...
- Installing databricks/databricks v1.86.0...
- Installing hashicorp/aws v5.0.0...
Terraform has been successfully initialized!
```

### 2. Validate Configuration

```bash
terraform validate
```

### 3. Plan Deployment

```bash
terraform plan -out=tfplan
```

This will show all resources to be created (approximately 40+ resources).

### 4. Apply Configuration

```bash
terraform apply tfplan
```

‚è±Ô∏è **Estimated deployment time**: 10-15 minutes

### 5. Verify Deployment

```bash
# Check outputs
terraform output

# Expected outputs:
# catalog_name           = "data_platform"
# metastore_id           = "..."
# data_engineering_cluster_id = "..."
# sql_warehouse_id       = "..."
# delta_lake_bucket      = "databricks-delta-lake-prod"
```

---

## üîß Configuration Options

### Customizing the Deployment

Edit `terraform.tfvars` to customize:

```hcl
# Environment
environment = "dev"  # or "staging", "prod"

# Catalog name
catalog_name = "my_data_platform"

# Cluster sizing
# (Edit databricks.tf to adjust autoscaling min/max)

# S3 bucket naming
# (Edit databricks.tf to customize bucket names)
```

### Multi-Environment Setup

```bash
# Development
terraform workspace new dev
terraform apply -var="environment=dev"

# Production
terraform workspace new prod
terraform apply -var="environment=prod"
```

---

## üìä Cost Estimation

Approximate monthly costs (AWS us-east-1, production):

| Resource | Configuration | Monthly Cost |
|----------|--------------|--------------|
| Databricks Workspace | Standard tier | $0 (pay-per-use) |
| Data Engineering Cluster | 2-200 nodes, on-demand | ~$5,000-$50,000 |
| Streaming Cluster | 2-50 nodes, always-on | ~$3,000-$15,000 |
| SQL Warehouse | 2X-Large serverless | ~$2,000-$10,000 |
| S3 Storage | 100TB Delta Lake | ~$2,300 |
| S3 Requests | API calls | ~$100 |
| Data Transfer | Inter-AZ/region | ~$500 |
| **Total** | | **~$12,900-$77,900/month** |

**Cost Optimization Tips**:
- Use spot instances for non-critical workloads
- Enable auto-termination on clusters
- Leverage S3 lifecycle policies (included)
- Use serverless SQL warehouses (included)
- Monitor with cost tracking dashboard (included)

---

## üîí Security Considerations

### Implemented Security Features

‚úÖ **Encryption**
- S3 server-side encryption (AES-256)
- In-transit encryption (TLS 1.3)

‚úÖ **Access Control**
- IAM roles with least privilege
- Unity Catalog RBAC
- Group-based permissions

‚úÖ **Audit**
- System schemas for audit logs
- Access logging enabled

‚úÖ **Network Security**
- VPC isolation (configure VPC in AWS)
- Security groups for clusters
- Private endpoints (optional)

### Additional Recommended Steps

1. **Enable S3 bucket policies**
   ```hcl
   # Add to databricks.tf
   resource "aws_s3_bucket_policy" "delta_lake" {
     # Restrict to Databricks IPs only
   }
   ```

2. **Configure VPC**
   ```hcl
   # Add VPC module
   module "vpc" {
     source = "terraform-aws-modules/vpc/aws"
     # ...
   }
   ```

3. **Enable CloudTrail logging**
   ```hcl
   resource "aws_cloudtrail" "databricks" {
     # ...
   }
   ```

---

## üß™ Testing

### 1. Smoke Test

After deployment, verify basic functionality:

```bash
# Test cluster connectivity
databricks clusters list

# Test SQL warehouse
databricks sql list-warehouses

# Test Unity Catalog
databricks catalogs list
```

### 2. Integration Test

Run the generated data migration job:

```bash
# Trigger job manually
databricks jobs run-now --job-id <JOB_ID>

# Monitor job execution
databricks jobs get-run --run-id <RUN_ID>
```

### 3. Validation Queries

```sql
-- Verify catalog exists
SHOW CATALOGS;

-- Verify schemas
USE CATALOG data_platform;
SHOW SCHEMAS;

-- Check permissions
SHOW GRANTS ON CATALOG data_platform;
```

---

## üîÑ Maintenance

### Updating Resources

1. Modify `databricks.tf` as needed
2. Run `terraform plan` to review changes
3. Apply with `terraform apply`

### Backup & State Management

```bash
# Backup Terraform state
aws s3 cp terraform.tfstate s3://your-backup-bucket/terraform/

# Use remote state (recommended)
terraform {
  backend "s3" {
    bucket = "terraform-state-bucket"
    key    = "databricks/terraform.tfstate"
    region = "us-east-1"
  }
}
```

### Monitoring Changes

Track infrastructure drift:

```bash
# Check for manual changes
terraform plan -refresh-only

# Detect configuration drift
terraform plan -detailed-exitcode
```

---

## üìà Scaling

### Horizontal Scaling

Increase cluster capacity:

```hcl
# In databricks.tf
resource "databricks_cluster" "data_engineering" {
  autoscale {
    min_workers = 5   # Increased from 2
    max_workers = 500 # Increased from 200
  }
}
```

### Vertical Scaling

Use larger instance types:

```hcl
resource "databricks_cluster" "data_engineering" {
  node_type_id = "i3.4xlarge"  # Upgraded from i3.xlarge
}
```

---

## üêõ Troubleshooting

### Common Issues

**Issue**: `Error: Workspace not found`
- **Solution**: Verify `workspace_host` in terraform.tfvars

**Issue**: `Error: Insufficient permissions`
- **Solution**: Check AWS IAM role has `databricks:*` permissions

**Issue**: `Error: Unity Catalog already exists`
- **Solution**: Import existing resource:
  ```bash
  terraform import databricks_metastore.this <METASTORE_ID>
  ```

**Issue**: `Error: S3 bucket already exists`
- **Solution**: Either import or rename bucket in configuration

---

## üìö Additional Resources

### Documentation
- [Databricks Terraform Provider](https://registry.terraform.io/providers/databricks/databricks/latest/docs)
- [Unity Catalog Setup Guide](https://docs.databricks.com/aws/en/dev-tools/terraform/automate-uc)
- [ArcLang Documentation](https://github.com/anthropics/arclang)

### Support
- Databricks Community: https://community.databricks.com
- Terraform Registry: https://registry.terraform.io
- ArcLang Issues: https://github.com/anthropics/arclang/issues

---

## üéâ Success Criteria

After successful deployment, you should have:

‚úÖ Unity Catalog metastore configured  
‚úÖ Data platform catalog with 7 schemas  
‚úÖ 3 compute resources (2 clusters + 1 SQL warehouse)  
‚úÖ Data migration ETL job scheduled  
‚úÖ RBAC configured for 3 user groups  
‚úÖ S3 storage with encryption and lifecycle policies  
‚úÖ Monitoring dashboard and alerts  
‚úÖ 100% traceability from requirements to resources

---

## üîÑ Regenerating from Updated Model

If you update the ArcLang model, regenerate Terraform:

```bash
cd /Users/malek/Arclang

# Update the model
vi examples/data_platform_migration.arc

# Recompile and validate
cargo run --release -- build examples/data_platform_migration.arc

# Regenerate Terraform
cargo run --release -- export examples/data_platform_migration.arc \
  -f terraform -o terraform/databricks_v2.tf

# Review changes
diff terraform/databricks.tf terraform/databricks_v2.tf

# Apply updates
cd terraform
terraform plan
terraform apply
```

---

## üìä Model Traceability

Every Terraform resource includes tags mapping back to ArcLang model:

```hcl
custom_tags = {
  component     = "LA-PROC-001"      # Logical component ID
  requirement   = "SYS-SCALE-001"    # System requirement ID
  safety_level  = "High"             # Safety classification
  environment   = var.environment    # Deployment environment
  managed_by    = "terraform"        # Infrastructure management
}
```

This ensures **100% traceability** from:
- **Requirements** ‚Üí **Components** ‚Üí **Infrastructure**

---

**Generated by**: ArcLang Terraform Generator  
**Model Version**: 1.0.0  
**License**: MIT  
**Maintainers**: Malek Baroudi & Bilel Laasami
